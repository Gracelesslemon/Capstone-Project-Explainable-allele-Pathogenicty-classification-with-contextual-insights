{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df805987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)  \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c124ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(r\"C:\\Users\\vigne\\Desktop\\Capstone\\datasets\\model_train_data.duckdb\")\n",
    "adf=con.execute(\"select * from allele\").fetch_df()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52eb81",
   "metadata": {},
   "source": [
    "# Majority Undersampling\n",
    "- We have way more benign comapared to pathogenic. \n",
    "- undersampling benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c779d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes first\n",
    "pathogenic_df = adf[adf['ClinicalSignificance'] == 1]\n",
    "benign_df = adf[adf['ClinicalSignificance'] == 0]\n",
    "benign_sampled = benign_df.sample(n=len(pathogenic_df), random_state=42)\n",
    "balanced_df = pd.concat([pathogenic_df, benign_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (exclude IDs and target)\n",
    "feature_cols_allele = [col for col in balanced_df.columns if col not in ['AlleleID', 'ClinicalSignificance','GeneID']]\n",
    "\n",
    "X = balanced_df[feature_cols_allele]\n",
    "y = balanced_df['ClinicalSignificance']\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "trainx, X_temp, trainy, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "valx, testx, valy, testy = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b673c8",
   "metadata": {},
   "source": [
    "# SENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872db70",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058a956",
   "metadata": {},
   "source": [
    "### Conceptizer : Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityConceptizer(nn.Module):\n",
    "    \"\"\"\n",
    "       - Does absolutely nothing conceptually - just adds then removes a dummy dimension\n",
    "       - Kept only for maintaining proper SENN interface/flow\n",
    "       - If in the future you want to try other conceptizers, the necessary structure is present\n",
    "       - Note: Makes reconstruction loss meaningless (always ~0 since recon_x == original input)\n",
    "   \"\"\"\n",
    "    def __init__(self, **kwargs) :\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return x.unsqueeze(-1)  # (BATCH, FEATURES, 1)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return z.squeeze(-1) # (BATCH, FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3995521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearParameterizer(nn.Module):\n",
    "    \"\"\"\n",
    "        - Hidden layers by default: 128, 64, 32 -> achieved 93% test accuracy\n",
    "        - Custom hidden_sizes can be provided for experimentation\n",
    "        - Takes raw input features (not concepts so -> called with x or concepts.squeeze) since IdentityConceptizer makes them equivalent\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_concepts, num_classes, hidden_sizes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.num_concepts = num_concepts\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Default hidden sizes if not provided\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [num_features, 128, 64, 32, num_concepts * num_classes]\n",
    "        else:\n",
    "            hidden_sizes = [num_features] + list(hidden_sizes) + [num_concepts * num_classes]\n",
    "        \n",
    "        layers = []\n",
    "        for h, h_next in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n",
    "            layers.append(nn.Linear(h, h_next))\n",
    "            if h_next != hidden_sizes[-1]:  \n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output.view(x.size(0), self.num_concepts, self.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a97318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumAggregator(nn.Module):\n",
    "    '''\n",
    "        - Aggregates concepts and relevances using weighted sum (batch matrix multiplication)\n",
    "        - Applies log_softmax for final class probability distribution\n",
    "    '''\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, concepts, relevances):\n",
    "        # concepts: (BATCH, NUM_CONCEPTS, 1)\n",
    "        # relevances: (BATCH, NUM_CONCEPTS, NUM_CLASSES)\n",
    "        aggregated = torch.bmm(relevances.permute(0, 2, 1), concepts).squeeze(-1)\n",
    "        return F.log_softmax(aggregated, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENN(nn.Module):\n",
    "    ''' \n",
    "        - With IdentityConceptizer: recon_x is identical to original input (reconstruction loss = 0)\n",
    "        - Returns: predictions, explanations=(concepts, relevances), reconstruction\n",
    "        - Explanations show which concepts are relevant for each class prediction\n",
    "    '''\n",
    "    def __init__(self, conceptizer, parameterizer, aggregator):\n",
    "        super().__init__()\n",
    "        self.conceptizer = conceptizer\n",
    "        self.parameterizer = parameterizer\n",
    "        self.aggregator = aggregator\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # recon_x is same as original data when using identity conceptizer.\n",
    "        concepts, recon_x = self.conceptizer(x)\n",
    "        relevances = self.parameterizer(x)\n",
    "        predictions = self.aggregator(concepts, relevances)\n",
    "        explanations = (concepts, relevances)\n",
    "        return predictions, explanations, recon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9ead6",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aff403",
   "metadata": {},
   "source": [
    "### Create dataloaders\n",
    "- batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X, y, batch_size=64, shuffle=True):\n",
    "    \"\"\"Convert pandas DataFrame to PyTorch DataLoader\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X.values)\n",
    "    y_tensor = torch.LongTensor(y.values)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f3250",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "- for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bac549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions, explanations, recon_x = model(data)\n",
    "        \n",
    "        # Main classification loss\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        # Reconstruction loss : is 0 since recon_x == data. \n",
    "        # this line is  kept as boilerplate code incase of trying out different conceptizers.\n",
    "        recon_loss = F.mse_loss(recon_x, data)\n",
    "        total_loss_val = loss + 0.01 * recon_loss  \n",
    "        \n",
    "        total_loss_val.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_loss_val.item()\n",
    "        pred = predictions.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
