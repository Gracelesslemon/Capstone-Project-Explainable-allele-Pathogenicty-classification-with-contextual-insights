{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df805987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)  \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c124ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(r\"C:\\Users\\vigne\\Desktop\\Capstone\\datasets\\model_train_data.duckdb\")\n",
    "adf=con.execute(\"select * from allele\").fetch_df()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52eb81",
   "metadata": {},
   "source": [
    "# Majority Undersampling\n",
    "- We have way more benign comapared to pathogenic. \n",
    "- undersampling benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c779d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes first\n",
    "pathogenic_df = adf[adf['ClinicalSignificance'] == 1]\n",
    "benign_df = adf[adf['ClinicalSignificance'] == 0]\n",
    "benign_sampled = benign_df.sample(n=len(pathogenic_df), random_state=42)\n",
    "balanced_df = pd.concat([pathogenic_df, benign_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (exclude IDs and target)\n",
    "feature_cols_allele = [col for col in balanced_df.columns if col not in ['AlleleID', 'ClinicalSignificance','GeneID']]\n",
    "\n",
    "X = balanced_df[feature_cols_allele]\n",
    "y = balanced_df['ClinicalSignificance']\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "trainx, X_temp, trainy, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "valx, testx, valy, testy = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b673c8",
   "metadata": {},
   "source": [
    "# SENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872db70",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058a956",
   "metadata": {},
   "source": [
    "### Conceptizer : Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityConceptizer(nn.Module):\n",
    "    \"\"\"\n",
    "       - Does absolutely nothing conceptually - just adds then removes a dummy dimension\n",
    "       - Kept only for maintaining proper SENN interface/flow\n",
    "       - If in the future you want to try other conceptizers, the necessary structure is present\n",
    "       - Note: Makes reconstruction loss meaningless (always ~0 since recon_x == original input)\n",
    "   \"\"\"\n",
    "    def __init__(self, **kwargs) :\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return x.unsqueeze(-1)  # (BATCH, FEATURES, 1)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return z.squeeze(-1) # (BATCH, FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3995521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearParameterizer(nn.Module):\n",
    "    \"\"\"\n",
    "        - Hidden layers by default: 128, 64, 32 -> achieved 93% test accuracy\n",
    "        - Custom hidden_sizes can be provided for experimentation\n",
    "        - Takes raw input features (not concepts so -> called with x or concepts.squeeze) since IdentityConceptizer makes them equivalent\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_concepts, num_classes, hidden_sizes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.num_concepts = num_concepts\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Default hidden sizes if not provided\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [num_features, 128, 64, 32, num_concepts * num_classes]\n",
    "        else:\n",
    "            hidden_sizes = [num_features] + list(hidden_sizes) + [num_concepts * num_classes]\n",
    "        \n",
    "        layers = []\n",
    "        for h, h_next in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n",
    "            layers.append(nn.Linear(h, h_next))\n",
    "            if h_next != hidden_sizes[-1]:  \n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
