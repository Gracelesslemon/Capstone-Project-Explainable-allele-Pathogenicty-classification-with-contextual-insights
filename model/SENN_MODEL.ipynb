{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df805987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)  \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c124ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(r\"C:\\Users\\vigne\\Desktop\\Capstone\\datasets\\model_train_data.duckdb\")\n",
    "adf=con.execute(\"select * from allele\").fetch_df()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52eb81",
   "metadata": {},
   "source": [
    "# Majority Undersampling\n",
    "- We have way more benign comapared to pathogenic. \n",
    "- undersampling benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c779d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes first\n",
    "pathogenic_df = adf[adf['ClinicalSignificance'] == 1]\n",
    "benign_df = adf[adf['ClinicalSignificance'] == 0]\n",
    "benign_sampled = benign_df.sample(n=len(pathogenic_df), random_state=42)\n",
    "balanced_df = pd.concat([pathogenic_df, benign_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (exclude IDs and target)\n",
    "feature_cols_allele = [col for col in balanced_df.columns if col not in ['AlleleID', 'ClinicalSignificance','GeneID']]\n",
    "\n",
    "X = balanced_df[feature_cols_allele]\n",
    "y = balanced_df['ClinicalSignificance']\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "trainx, X_temp, trainy, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "valx, testx, valy, testy = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx.drop(['ref_is_A', 'ref_is_T', 'ref_is_G', 'ref_is_C', 'alt_is_A', 'alt_is_T', 'alt_is_G', 'alt_is_C',\n",
    "       'chr_11', 'chr_6', 'chr_2', 'chr_20', 'chr_10', 'chr_16', 'chr_22',\n",
    "       'chr_15', 'chr_1', 'chr_7', 'chr_8', 'chr_14', 'chr_21', 'chr_5',\n",
    "       'chr_4', 'chr_19', 'chr_3', 'chr_17', 'chr_12', 'chr_18', 'chr_9',\n",
    "       'chr_13', 'chr_MT', 'chr_Y', 'chr_X'],axis = 1 , inplace = True)\n",
    "\n",
    "valx.drop(['ref_is_A', 'ref_is_T', 'ref_is_G', 'ref_is_C', 'alt_is_A', 'alt_is_T', 'alt_is_G', 'alt_is_C',\n",
    "       'chr_11', 'chr_6', 'chr_2', 'chr_20', 'chr_10', 'chr_16', 'chr_22',\n",
    "       'chr_15', 'chr_1', 'chr_7', 'chr_8', 'chr_14', 'chr_21', 'chr_5',\n",
    "       'chr_4', 'chr_19', 'chr_3', 'chr_17', 'chr_12', 'chr_18', 'chr_9',\n",
    "       'chr_13', 'chr_MT', 'chr_Y', 'chr_X'],axis = 1 , inplace = True)\n",
    "\n",
    "testx.drop(['ref_is_A', 'ref_is_T', 'ref_is_G', 'ref_is_C', 'alt_is_A', 'alt_is_T', 'alt_is_G', 'alt_is_C',\n",
    "       'chr_11', 'chr_6', 'chr_2', 'chr_20', 'chr_10', 'chr_16', 'chr_22',\n",
    "       'chr_15', 'chr_1', 'chr_7', 'chr_8', 'chr_14', 'chr_21', 'chr_5',\n",
    "       'chr_4', 'chr_19', 'chr_3', 'chr_17', 'chr_12', 'chr_18', 'chr_9',\n",
    "       'chr_13', 'chr_MT', 'chr_Y', 'chr_X'],axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b673c8",
   "metadata": {},
   "source": [
    "# SENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872db70",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058a956",
   "metadata": {},
   "source": [
    "### Conceptizer : Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityConceptizer(nn.Module):\n",
    "    \"\"\"\n",
    "       - Does absolutely nothing conceptually - just adds then removes a dummy dimension\n",
    "       - Kept only for maintaining proper SENN interface/flow\n",
    "       - If in the future you want to try other conceptizers, the necessary structure is present\n",
    "       - Note: Makes reconstruction loss meaningless (always ~0 since recon_x == original input)\n",
    "   \"\"\"\n",
    "    def __init__(self, **kwargs) :\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return x.unsqueeze(-1)  # (BATCH, FEATURES, 1)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return z.squeeze(-1) # (BATCH, FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3995521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearParameterizer(nn.Module):\n",
    "    \"\"\"\n",
    "        - Hidden layers by default: 128, 64, 32 -> achieved 93% test accuracy\n",
    "        - Custom hidden_sizes can be provided for experimentation\n",
    "        - Takes raw input features (not concepts so -> called with x or concepts.squeeze) since IdentityConceptizer makes them equivalent\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_concepts, num_classes, hidden_sizes=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.num_concepts = num_concepts\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Default hidden sizes if not provided\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [num_features, 128, 64, 32, num_concepts * num_classes]\n",
    "        else:\n",
    "            hidden_sizes = [num_features] + list(hidden_sizes) + [num_concepts * num_classes]\n",
    "        \n",
    "        layers = []\n",
    "        for h, h_next in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n",
    "            layers.append(nn.Linear(h, h_next))\n",
    "            if h_next != hidden_sizes[-1]:  \n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output.view(x.size(0), self.num_concepts, self.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a97318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumAggregator(nn.Module):\n",
    "    '''\n",
    "        - Aggregates concepts and relevances using weighted sum (batch matrix multiplication)\n",
    "        - Applies log_softmax for final class probability distribution\n",
    "    '''\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, concepts, relevances):\n",
    "        # concepts: (BATCH, NUM_CONCEPTS, 1)\n",
    "        # relevances: (BATCH, NUM_CONCEPTS, NUM_CLASSES)\n",
    "        aggregated = torch.bmm(relevances.permute(0, 2, 1), concepts).squeeze(-1)\n",
    "        return F.log_softmax(aggregated, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENN(nn.Module):\n",
    "    ''' \n",
    "        - With IdentityConceptizer: recon_x is identical to original input (reconstruction loss = 0)\n",
    "        - Returns: predictions, explanations=(concepts, relevances), reconstruction\n",
    "        - Explanations show which concepts are relevant for each class prediction\n",
    "    '''\n",
    "    def __init__(self, conceptizer, parameterizer, aggregator):\n",
    "        super().__init__()\n",
    "        self.conceptizer = conceptizer\n",
    "        self.parameterizer = parameterizer\n",
    "        self.aggregator = aggregator\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # recon_x is same as original data when using identity conceptizer.\n",
    "        concepts, recon_x = self.conceptizer(x)\n",
    "        relevances = self.parameterizer(x)\n",
    "        predictions = self.aggregator(concepts, relevances)\n",
    "        explanations = (concepts, relevances)\n",
    "        return predictions, explanations, recon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9ead6",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aff403",
   "metadata": {},
   "source": [
    "### Create dataloaders\n",
    "- batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X, y, batch_size=64, shuffle=True):\n",
    "    \"\"\"Convert pandas DataFrame to PyTorch DataLoader\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X.values)\n",
    "    y_tensor = torch.LongTensor(y.values)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f3250",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "- for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bac549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions, explanations, recon_x = model(data)\n",
    "        \n",
    "        # Main classification loss\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        # Reconstruction loss : is 0 since recon_x == data. \n",
    "        # this line is  kept as boilerplate code incase of trying out different conceptizers.\n",
    "        recon_loss = F.mse_loss(recon_x, data)\n",
    "        total_loss_val = loss + 0.01 * recon_loss  \n",
    "        \n",
    "        total_loss_val.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_loss_val.item()\n",
    "        pred = predictions.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8562a9d",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a00e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            predictions, explanations, recon_x = model(data)\n",
    "            \n",
    "            loss = criterion(predictions, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = predictions.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0bd4ca",
   "metadata": {},
   "source": [
    "### Main Training\n",
    "- Dropout : 0.3\n",
    "- Criterion : NLL\n",
    "- Optimizer : Adam\n",
    "- scheduler :\n",
    "    - reduce\n",
    "    - 30 epochs\n",
    "    - gamma = 0.5\n",
    "- patience counter added if the model plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_senn_model(trainx, trainy, valx, valy, \n",
    "                     num_epochs=100, batch_size=64, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Complete SENN training pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trainx, trainy : Training data \n",
    "    valx, valy : Validation data  \n",
    "    testx, testy : Test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data dimensions\n",
    "    num_features = trainx.shape[1]\n",
    "    num_concepts = num_features  # Each feature is a concept\n",
    "    num_classes = 2  # Binary classification\n",
    "    \n",
    "    print(f\"Number of features: {num_features}\")\n",
    "    print(f\"Number of concepts: {num_concepts}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = create_data_loader(trainx, trainy, batch_size, shuffle=True)\n",
    "    val_loader = create_data_loader(valx, valy, batch_size, shuffle=False)\n",
    "    test_loader = create_data_loader(testx, testy, batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize SENN components\n",
    "    conceptizer = IdentityConceptizer()\n",
    "    parameterizer = LinearParameterizer(\n",
    "        num_features=num_features,\n",
    "        num_concepts=num_concepts, \n",
    "        num_classes=num_classes,\n",
    "        hidden_sizes=[128, 64, 32],  # change if testing\n",
    "        dropout=0.3\n",
    "    )\n",
    "    aggregator = SumAggregator(num_classes=num_classes)\n",
    "    \n",
    "    # Create SENN model\n",
    "    model = SENN(conceptizer, parameterizer, aggregator)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.NLLLoss()  # For log_softmax output\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), r'C:\\Users\\vigne\\Desktop\\Capstone\\modelsbest_senn_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            print('-' * 50)\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break   \n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0888d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model = train_senn_model(trainx, trainy, valx, valy ,num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e39d6",
   "metadata": {},
   "source": [
    "# OUTPUT Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, feature_names, device, sample_input=None, class_names=(\"benign\", \"pathogenic\")):\n",
    "    \"\"\"Extract feature importance from SENN model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if sample_input is None:\n",
    "        sample_input = torch.randn(1, len(feature_names)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions, explanations, recon_x = model(sample_input)\n",
    "        concepts, relevances = explanations\n",
    "        # relevances: [batch, num_concepts, num_classes]\n",
    "\n",
    "        # Global average (across batch and classes)\n",
    "        global_avg = relevances.mean(dim=(0, 2)).cpu().numpy()\n",
    "\n",
    "        # Per-class average (across batch only)\n",
    "        per_class_avg = relevances.mean(dim=0).cpu().numpy()  # [num_concepts, num_classes]\n",
    "\n",
    "    # Build dictionary\n",
    "    importance_dict = {}\n",
    "    for i, name in enumerate(feature_names):\n",
    "        importance_dict[name] = {\n",
    "            \"global\": float(global_avg[i]),\n",
    "            class_names[0]: float(per_class_avg[i, 0]),  # benign\n",
    "            class_names[1]: float(per_class_avg[i, 1])   # pathogenic\n",
    "        }\n",
    "\n",
    "    return importance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance_rankings(feature_importance):\n",
    "    \"\"\"Print top 10 features for global, benign, pathogenic\"\"\"\n",
    "    \n",
    "    # --- Global ---\n",
    "    sorted_global = sorted(feature_importance.items(),\n",
    "                           key=lambda x: abs(x[1][\"global\"]),\n",
    "                           reverse=True)\n",
    "    print(\"\\nTop 10 Most Important Features (Global):\")\n",
    "    for i, (feature, scores) in enumerate(sorted_global[:10]):\n",
    "        print(f\"{i+1:2d}. {feature:<40} | global: {scores['global']:>8.4f}\")\n",
    "    \n",
    "    # --- Benign ---\n",
    "    sorted_benign = sorted(feature_importance.items(),\n",
    "                           key=lambda x: abs(x[1][\"benign\"]),  # Fixed: using \"benign\" key\n",
    "                           reverse=True)\n",
    "    print(\"\\nTop 10 Most Important Features (Benign):\")\n",
    "    for i, (feature, scores) in enumerate(sorted_benign[:10]):\n",
    "        print(f\"{i+1:2d}. {feature:<40} | benign: {scores['benign']:>8.4f}\")\n",
    "    \n",
    "    # --- Pathogenic ---\n",
    "    sorted_pathogenic = sorted(feature_importance.items(),\n",
    "                               key=lambda x: abs(x[1][\"pathogenic\"]),  # Fixed: using \"pathogenic\" key\n",
    "                               reverse=True)\n",
    "    print(\"\\nTop 10 Most Important Features (Pathogenic):\")\n",
    "    for i, (feature, scores) in enumerate(sorted_pathogenic[:10]):\n",
    "        print(f\"{i+1:2d}. {feature:<40} | pathogenic: {scores['pathogenic']:>8.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb289347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_concepts(feature_importance, feature_names):\n",
    "    \"\"\"Group features into concepts\"\"\"\n",
    "    concept_groups = {\n",
    "        'genomic_location': ['chr_', 'is_genomic', 'is_mitochondrial'],\n",
    "        'sequence_change': ['ref_is_', 'alt_is_'], \n",
    "        'gene_context': ['has_VariantGeneRelation_'],\n",
    "        'molecular_consequence': ['has_MC_'],\n",
    "        'data_source': ['has_Origin_']\n",
    "    }\n",
    "    \n",
    "    concept_importance = {}\n",
    "    detailed_contributions = {}\n",
    "    \n",
    "    valid_features = {f: feature_importance[f] for f in feature_names if f in feature_importance}\n",
    "    \n",
    "    for concept_name, feature_prefixes in concept_groups.items():\n",
    "        concept_features = {}\n",
    "        total_importance = 0\n",
    "        \n",
    "        for feature_name, scores in valid_features.items():\n",
    "            for prefix in feature_prefixes:\n",
    "                if feature_name.startswith(prefix):\n",
    "                    concept_features[feature_name] = scores\n",
    "                    total_importance += abs(scores[\"global\"])\n",
    "                    break\n",
    "        \n",
    "        concept_importance[concept_name] = total_importance\n",
    "        detailed_contributions[concept_name] = concept_features\n",
    "    \n",
    "    return concept_importance, detailed_contributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_concept_importance(concept_importance, detailed_contributions):\n",
    "    \"\"\"Print concept-level importance\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONCEPT-LEVEL IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nConcept-Level Importance (Global):\")\n",
    "    for concept, importance in sorted(concept_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{concept:<25}: {importance:>8.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Feature Contributions by Concept:\")\n",
    "    for concept_name, features in detailed_contributions.items():\n",
    "        if features:\n",
    "            print(f\"\\n{concept_name.upper()}:\")\n",
    "            sorted_features = sorted(features.items(),\n",
    "                                     key=lambda x: abs(x[1][\"global\"]),\n",
    "                                     reverse=True)\n",
    "            for feature, scores in sorted_features[:5]:  # Top 5 per concept\n",
    "                print(f\"  {feature:<30}: \"\n",
    "                      f\"global={scores['global']:>6.3f}, \"\n",
    "                      f\"benign={scores['benign']:>6.3f}, \"  # Fixed: using \"benign\" key\n",
    "                      f\"pathogenic={scores['pathogenic']:>6.3f}\")  # Fixed: using \"pathogenic\" key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b87882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_result(model, testx, testy, feature_names, batch_size=64):\n",
    "    \"\"\"\n",
    "    Complete SENN analysis: testing, feature importance, and concept grouping\n",
    "\n",
    "    Args:\n",
    "        model: Trained SENN model \n",
    "        testx, testy: Test data\n",
    "        feature_names: List of feature names (e.g., trainx.columns.tolist())\n",
    "        batch_size: Batch size for testing\n",
    "\n",
    "    Returns:\n",
    "        test_results, feature_importance, concept_importance, detailed_contributions\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(r'C:\\Users\\vigne\\Desktop\\Capstone\\models\\best_senn_model.pth'))\n",
    "\n",
    "    # Test evaluation\n",
    "    test_loader = create_data_loader(testx, testy, batch_size, shuffle=False)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    test_loss, test_acc, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(test_targets, test_preds, target_names=['Benign', 'Pathogenic']))\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = get_feature_importance(model, feature_names, device)\n",
    "\n",
    "    # Print individual feature importance\n",
    "    print_feature_importance_rankings(feature_importance)\n",
    "\n",
    "    # Group into concepts\n",
    "    concept_importance, detailed_contributions = group_concepts(feature_importance, feature_names)\n",
    "\n",
    "    # Print concept-level results\n",
    "    print_concept_importance(concept_importance, detailed_contributions)\n",
    "\n",
    "    test_results = (test_loss, test_acc, test_preds, test_targets)\n",
    "    return test_results, feature_importance, concept_importance, detailed_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results, feature_importance, concept_importance, detailed_contributions = final_result(\n",
    "    model, testx, testy, trainx.columns.tolist()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
